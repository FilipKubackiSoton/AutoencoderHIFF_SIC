{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from example_lorenz import get_lorenz_data\n",
    "from autoencoderTF2 import Autoencoder, Encoder, Decoder\n",
    "from SINDYTF2 import sindy_library_tf, library_size\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Autoencoder\n",
    "    Stack both encoder and decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "    widths : List[int] = [32,28,25], \n",
    "    ekwargs : Optional[dict] = {}, \n",
    "    dkwargs : Optional[dict] = {}, \n",
    "    **kwargs):\n",
    "        if not (\"name\" in kwargs.keys()):\n",
    "            kwargs[\"name\"] = \"autoencoder\"\n",
    "\n",
    "        super(Autoencoder, self).__init__(**kwargs)\n",
    "        self.input_dim = widths[0]\n",
    "        self.latent_dim = widths[-1]\n",
    "        self.encoder = Encoder(widths, **ekwargs).build_graph()\n",
    "        self.decoder = Decoder(self.encoder, **dkwargs).build_graph()\n",
    "        self.theta = None\n",
    "    \n",
    "    def call(self, input):        \n",
    "        x = self.encoder.layers[1](input)\n",
    "        for layer in self.encoder.layers[2:] + self.decoder.layers[1:]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super(Autoencoder, self).compile(**kwargs)\n",
    "\n",
    "    def build_graph(self,):\n",
    "        x = Input(shape=(self.input_dim, ), name = 'autoencoder_input')\n",
    "        return tf.keras.Model(inputs = [x], outputs = self.call(x))\n",
    "\n",
    "    def set_theta(self, x, poly_order, include_sin):\n",
    "        self.theta = sindy_library_tf(self.encode(x), poly_order, include_sin)\n",
    "\n",
    "    def encode(self, input):\n",
    "        return self.encoder(input)\n",
    "    \n",
    "    def decode(self, input):\n",
    "        return self.decoder(input)\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.input_dim\n",
    "    \n",
    "    def get_latent_dim(self) -> int:\n",
    "        return self.latent_dim\n",
    "    \n",
    "    def set_theta(self, theta):\n",
    "        self.theta = theta\n",
    "\n",
    "    def get_loss(self, x):\n",
    "        x = self.currentState[\"x\"]\n",
    "        x_decode = self.currentState[\"x_decode\"]\n",
    "        dz = self.currentState[\"dz\"]\n",
    "        dz_predict = self.currentState[\"dz_predict\"]\n",
    "        dx = self.currentState[\"dx\"]\n",
    "        dx_decode = self.currentState[\"dx_decode\"]\n",
    "        sindy_coefficients = self.coefficient_mask*self.sindy_coefficients\n",
    "\n",
    "        losses = {}\n",
    "        losses['decoder'] = tf.reduce_mean((x - x_decode)**2)\n",
    "        losses['sindy_z'] = tf.reduce_mean((dz - dz_predict)**2)\n",
    "        losses['sindy_x'] = tf.reduce_mean((dx - dx_decode)**2)\n",
    "        losses['sindy_regularization'] = tf.reduce_mean(tf.abs(sindy_coefficients))\n",
    "        loss = self.currentState['loss_weight_decoder'] * losses['decoder'] \\\n",
    "           + self.currentState['loss_weight_sindy_z'] * losses['sindy_z'] \\\n",
    "           + self.currentState['loss_weight_sindy_x'] * losses['sindy_x'] \\\n",
    "           + self.currentState['loss_weight_sindy_regularization'] * losses['sindy_regularization']\n",
    "\n",
    "        loss_refinement = self.currentState['loss_weight_decoder'] * losses['decoder'] \\\n",
    "            + self.currentState['loss_weight_sindy_z'] * losses['sindy_z'] \\\n",
    "            + self.currentState['loss_weight_sindy_x'] * losses['sindy_x']\n",
    "\n",
    "        return loss, losses, loss_refinement\n",
    "    \n",
    "    def get_loss_refinement(self, ):\n",
    "        return 0\n",
    "    \n",
    "    def get_losses(self,) -> dict:\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SindyTrain:\n",
    "    \n",
    "    def __init__(self,\n",
    "    model : Autoencoder,\n",
    "    coefficient_threshold : float = 0.1,\n",
    "    threshold_frequency : int = 50,\n",
    "    sindyLossFn : tf.keras.losses.Loss =  \n",
    "    loss_weights : List[float] = [1.0, 0.0, 1e-4, 1e-5],\n",
    "    ):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            model (Autoencoder): Autoencoder not compiled\n",
    "            coefficient_threshold (float, optional): Defaults to 0.1.\n",
    "            threshold_frequency (int, optional): Defaults to 50.\n",
    "        \"\"\"\n",
    "        \n",
    "        # SINDY related attributes\n",
    "        self.model = model\n",
    "        self.coefficient_threshold = coefficient_threshold\n",
    "        self.threshold_frequency = threshold_frequency\n",
    "        self.loss_weight_decoder = loss_weights[0]\n",
    "        self.loss_weight_sindy_x = loss_weights[1]\n",
    "        self.loss_weight_sindy_z = loss_weights[2]\n",
    "        self.loss_weight_sindy_regularization = loss_weights[3]\n",
    "\n",
    "\n",
    "        self.currentState = {}\n",
    "        self.coefficient_mask = None\n",
    "        self.theta = None\n",
    "        self.coefficient_mask = None\n",
    "        self.sindy_coefficients = None\n",
    "        self.library_dim = None\n",
    " \n",
    "    @tf.function\n",
    "    def train_step(self, x, dx):\n",
    "        with tf.GradientTape() as tape:\n",
    "            dz = self.model.encode(dx, training=True)\n",
    "            dz_predict = self.sindy_predict()\n",
    "            x_decode = self.model(x, training=True)\n",
    "            dx_decode = self.model.decode(dz_predict, training=True)\n",
    "            \n",
    "            logits = self.model(x, training=True)\n",
    "            loss_value = self.get_loss(x, dx)\n",
    "        grads = tape.gradient(loss_value, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        # self.train_acc_metric.update_state(y, logits)\n",
    "        return loss_value\n",
    "\n",
    "    def refinement_step(self, x):\n",
    "        return \n",
    "\n",
    "    def fit(self, \n",
    "        optimizer : tf.keras.optimizers = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "        batch_size : int = 1024,\n",
    "        refinement_epochs = 1001,\n",
    "        data_path : str = os.getcwd() + \"/\"):\n",
    "        return\n",
    "\n",
    "    def get_loss(self, x, dx, x_decode, dx_decode, dz, dz_predict):\n",
    "        loss_refinement, losses = self.get_loss_refinement(x, dx)\n",
    "        return loss_refinement + self.loss_weight_sindy_regularization*losses['sindy_regularization'], losses\n",
    "\n",
    "    def get_loss_refinement(self, x, dx, x_decode, dx_decode, dz, dz_predict):\n",
    "\n",
    "        \"\"\"\n",
    "        Note: This method should not be called directly. \n",
    "        It is only meant to be overridden when subclassing tf.keras.Model. \n",
    "        To call a model on an input, always use the __call__() method, i.e. model(inputs), \n",
    "        which relies on the underlying call() method.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        losses = {}\n",
    "        losses['decoder'] = tf.reduce_mean(tf.math.math_ops.square(x - x_decode), axis=-1)\n",
    "        losses['sindy_z'] = tf.reduce_mean(tf.math.math_ops.square(dz - dz_predict), axis=-1)\n",
    "        losses['sindy_x'] = tf.reduce_mean(tf.math.math_ops.square(dx - dx_decode), axis=-1)\n",
    "        losses['sindy_regularization'] = tf.reduce_mean(tf.abs(self.sindy_coefficients))\n",
    "\n",
    "        loss_refinement = self.loss_weight_decoder * losses['decoder'] \\\n",
    "            + self.loss_weight_sindy_x * losses['sindy_z'] \\\n",
    "            + self.loss_weight_sindy_z * losses['sindy_x'] \\\n",
    "            \n",
    "        return loss_refinement, losses\n",
    "        \n",
    "\n",
    "    def set_theta(self, x, poly_order : Optional[int] = 3, include_sin : Optional[bool] = True):\n",
    "        self.theta = sindy_library_tf(x, self.model.latent_dim, poly_order, include_sin)\n",
    "        self.library_dim = library_size(self.model.latent_dim, poly_order, include_sin, True)\n",
    "        self.coefficient_mask = np.ones((self.library_dim, self.model.latent_dim))\n",
    "\n",
    "    def sindy_predict(self, ):\n",
    "        return tf.matmul(self.theta, self.coefficient_mask * self.sindy_coefficients)\n",
    "\n",
    "\n",
    "    def compile(self, ):\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SINDYLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, loss_weight_decoder, loss_weight_sindy_x, loss_weight_sindy_z, loss_weight_sindy_regularization):\n",
    "        self.loss_weight_decoder = loss_weight_decoder\n",
    "        self.loss_weight_sindy_x = loss_weight_sindy_x\n",
    "        self.loss_weight_sindy_z = loss_weight_sindy_z\n",
    "        self.loss_weight_sindy_regularization = loss_weight_sindy_regularization\n",
    "\n",
    "    def call(self, x, dx, x_decode, dx_decode, dz, dz_predict):\n",
    "        \"\"\"\n",
    "        Casting might not be needed\n",
    "        x_pred = tf.convert_to_tensor_v2(x_pred)\n",
    "        x_true = tf.cast(x_true, x_pred.dtype)\n",
    "        \"\"\"\n",
    "\n",
    "        losses = {}\n",
    "        losses['decoder'] = tf.losses.MeanSquaredError(x, x_decode) # tf.reduce_mean(tf.math.math_ops.square(x - x_decode), axis=-1)\n",
    "        losses['sindy_z'] = tf.losses.MeanSquaredError(dz, dz_predict) # tf.reduce_mean(tf.math.math_ops.square(dz - dz_predict), axis=-1)\n",
    "        losses['sindy_x'] = tf.losses.MeanSquaredError(dx, dx_decode) # tf.reduce_mean(tf.math.math_ops.square(dx - dx_decode), axis=-1)\n",
    "        losses['sindy_regularization'] = tf.reduce_mean(tf.abs(self.sindy_coefficients))\n",
    "        \n",
    "        loss_refinement = self.loss_weight_decoder * losses['decoder'] \\\n",
    "            + self.loss_weight_sindy_x * losses['sindy_z'] \\\n",
    "            + self.loss_weight_sindy_z * losses['sindy_x'] \\\n",
    "                \n",
    "        loss = loss_refinement \\\n",
    "            + self.loss_weight_sindy_regularization*losses['sindy_regularization']\n",
    "\n",
    "        return loss, loss_refinement, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SINDYRefinementLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, loss_weight_decoder, loss_weight_sindy_x, loss_weight_sindy_z, loss_weight_sindy_regularization):\n",
    "        self.loss_weight_decoder = loss_weight_decoder\n",
    "        self.loss_weight_sindy_x = loss_weight_sindy_x\n",
    "        self.loss_weight_sindy_z = loss_weight_sindy_z\n",
    "        self.loss_weight_sindy_regularization = loss_weight_sindy_regularization\n",
    "\n",
    "    def call(self, x, dx, x_decode, dx_decode, dz, dz_predict):\n",
    "        losses = {}\n",
    "        losses['decoder'] = tf.reduce_mean(tf.math.math_ops.square(x - x_decode), axis=-1)\n",
    "        losses['sindy_z'] = tf.reduce_mean(tf.math.math_ops.square(dz - dz_predict), axis=-1)\n",
    "        losses['sindy_x'] = tf.reduce_mean(tf.math.math_ops.square(dx - dx_decode), axis=-1)\n",
    "        losses['sindy_regularization'] = tf.reduce_mean(tf.abs(self.sindy_coefficients))\n",
    "\n",
    "        loss_refinement = self.loss_weight_decoder * losses['decoder'] \\\n",
    "            + self.loss_weight_sindy_x * losses['sindy_z'] \\\n",
    "            + self.loss_weight_sindy_z * losses['sindy_x'] \\\n",
    "            \n",
    "        return loss_refinement, losses\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
