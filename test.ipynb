{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import Autoencoder\n",
    "from SINDY import sindy_library_tf\n",
    "from HIFF import generate_training_sat\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] generating trainnig dataset...\n",
      "[INFO] generating testing dataset...\n"
     ]
    }
   ],
   "source": [
    "epochs = 5 \n",
    "batch_size = 16\n",
    "# generate the trainig set \n",
    "print(\"[INFO] generating trainnig dataset...\")\n",
    "(trainX, trainY) = generate_training_sat(32, 50)\n",
    "\n",
    "# generate the test set \n",
    "print(\"[INFO] generating testing dataset...\")\n",
    "(testX, testY) = generate_training_sat(32,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "autoencoder_input (InputLaye [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "encoder_0_dropout (Dropout)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 28)                924       \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 26)                754       \n",
      "_________________________________________________________________\n",
      "encoder_2_dropout (Dropout)  (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 24)                648       \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 20)                500       \n",
      "_________________________________________________________________\n",
      "decoder_0 (DenseTranspose)   (None, 24)                524       \n",
      "_________________________________________________________________\n",
      "decoder_1 (DenseTranspose)   (None, 26)                674       \n",
      "_________________________________________________________________\n",
      "decoder_2 (DenseTranspose)   (None, 28)                782       \n",
      "_________________________________________________________________\n",
      "decoder_3 (DenseTranspose)   (None, 32)                956       \n",
      "=================================================================\n",
      "Total params: 2,936\n",
      "Trainable params: 2,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 1s 4ms/step - loss: 4.0610 - mae: 1.4927\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.9679 - mae: 1.0485\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.7896 - mae: 0.9927\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 1.4406 - mae: 0.8844\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.1295 - mae: 0.7824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faeb8e53a58>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoencoder([32, 28, 26, 24, 20])\n",
    "model.build_graph().summary()\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.fit(trainX, trainX, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-50388e8d1979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'summary'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faeba2c9780>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(model.encoder.weights[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'l1': 0.001, 'l2': 0.0, 'dropout': 0.2, 'activation': 'tanh'},\n",
       " {'l1': 0.0, 'l2': 0.0, 'dropout': 0.0, 'activation': 'tanh'},\n",
       " {'l1': 0.01, 'l2': 0.0, 'dropout': 0.2, 'activation': 'tanh'},\n",
       " {'l1': 0.0, 'l2': 0.0, 'dropout': 0.0, 'activation': 'tanh'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def suplement_layers_params(architecture : List[int], layers_params : dict, layers_default_params : dict) -> dict:\n",
    "    # add empty dicionaries to layers_params to corecponds to the encoding_layers_size size\n",
    "    for _ in range(len(architecture) - len(layers_params)):\n",
    "        layers_params.append({})\n",
    "    return [{**layers_default_params, **x} for x in layers_params]\n",
    "\n",
    "\n",
    "widths = [32,28,24,18]\n",
    "layers_params = [{'l1' : 0.001, 'l2' : 0.0, 'dropout' : 0.2}, {}, {'l1' : 0.01, 'dropout' : 0.2}]\n",
    "layers_default_params = {'l1' : 0.0, 'l2' : 0.0, 'dropout' : 0.0, 'activation' : \"tanh\"}\n",
    "suplement_layers_params(widths, layers_params, layers_default_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not a mapping",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4f0bc04cd78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlayers_default_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not a mapping"
     ]
    }
   ],
   "source": [
    "{**layers_default_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Flatten\n",
    "from typing import List, Optional\n",
    "\n",
    "def suplement_layers_params(architecture : List[int], layers_params : dict, layers_default_params : dict) -> dict:\n",
    "    # add empty dicionaries to layers_params to corecponds to the encoding_layers_size size\n",
    "    for _ in range(len(architecture) - len(layers_params)):\n",
    "        layers_params.append({})\n",
    "    return [{**layers_default_params, **x} for x in layers_params]\n",
    "\n",
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Dense transpose layer from dense layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dense, activation = None, **kwargs):\n",
    "        self.dense = dense \n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        super(DenseTranspose, self).__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.b = self.add_weight(name= \"bias\", shape = [ self.dense.input_shape[-1]], initializer = \"zeros\")\n",
    "        self.w = self.dense.weights[0]\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z = tf.linalg.matmul(inputs, self.w, transpose_b = True)\n",
    "        return self.activation(z + self.b)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {\"w\": np.shape(tf.transpose(self.w))}    \n",
    "    @property \n",
    "    def weights_transpose(self):\n",
    "        return tf.transpose(self.dense.weights[0])\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "\n",
    "    isFirstInputLayer - use input dimension only in Input Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "    widths : List[int] = [32,24], \n",
    "    isFirstInputLayer : Optional[bool] = True,\n",
    "    layers_params : Optional[List[dict]] = [\n",
    "        {'l1' : 0.001, 'l2' : 0.0, 'dropout' : 0.2}, \n",
    "        {},\n",
    "        {\"dropout\" : 0.5}\n",
    "        ],\n",
    "    layers_default_params : dict = {\n",
    "        'l1' : 0.0, \n",
    "        'l2' : 0.0, \n",
    "        'dropout' : 0.0, \n",
    "        'activation' : \"tanh\",\n",
    "        'kernel_initializer' : tf.keras.initializers.GlorotUniform(),\n",
    "        'bias_initializer' : tf.keras.initializers.Zeros()\n",
    "        },\n",
    "    **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.latent_dim = widths[-1]\n",
    "        self.input_dim = widths[0]\n",
    "        self.layers = []\n",
    "        \n",
    "        if not (\"name\" in kwargs.keys()):\n",
    "            kwargs[\"name\"] = \"encoder\"\n",
    "\n",
    "        layers_params = suplement_layers_params(widths, layers_params, layers_default_params)\n",
    "        \n",
    "        for (layer_index, layer_dim), layer_params_ in zip(\n",
    "            enumerate(widths[1:] if isFirstInputLayer else widths), \n",
    "            layers_params):\n",
    "\n",
    "            if layer_params_[\"dropout\"] > 0.0:\n",
    "                self.layers.append(Dropout(\n",
    "                    layer_params_[\"dropout\"],\n",
    "                    name = kwargs[\"name\"] + \"_{}_dropout\".format(layer_index)\n",
    "                    ))\n",
    "            # construct encoder layer \n",
    "            self.layers.append(Dense(\n",
    "                    units = layer_dim,\n",
    "                    kernel_initializer= layer_params_[\"kernel_initializer\"],\n",
    "                    bias_initializer= layer_params_[\"bias_initializer\"],\n",
    "                    kernel_regularizer = tf.keras.regularizers.L1L2(\n",
    "                        l1=layer_params_[\"l1\"], \n",
    "                        l2=layer_params_[\"l2\"]),\n",
    "                    name = kwargs[\"name\"] + \"_{}\".format(layer_index)))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = Input(shape=(self.input_dim, ), name = 'encoder_input')\n",
    "        return tf.keras.Model(inputs = [x], outputs = self.call(x))\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Decoder\n",
    "\n",
    "    isTranspose - use transposed layers from encoder. If False create fresh layers of the same size of encoder\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Add decoder documentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "    encoder : Encoder, \n",
    "    isTranspose : Optional[bool] = True,  \n",
    "    layers_params : Optional[List[dict]] = [],\n",
    "    layers_default_params : dict = {\n",
    "        'l1' : 0.0, \n",
    "        'l2' : 0.0,\n",
    "        'dropout' : 0.0, \n",
    "        'activation' : \"tanh\",\n",
    "        'kernel_initializer' : tf.keras.initializers.GlorotUniform(),\n",
    "        'bias_initializer' : tf.keras.initializers.Zeros()\n",
    "        }, \n",
    "    **kwargs):\n",
    "\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.input_dim = encoder.weights[0].shape[-1]\n",
    "        self.output_dim = encoder.weights[-1].shape[-1]\n",
    "        self.layers = []\n",
    "\n",
    "        if not (\"name\" in kwargs.keys()):\n",
    "            kwargs[\"name\"] = \"decoder\"\n",
    "\n",
    "        layers_params = suplement_layers_params(widths, layers_params, layers_default_params)\n",
    "\n",
    "        if isTranspose:\n",
    "            for layer_index, layer in enumerate([x for x in encoder.layers[1:][::-1] if type(x) == Dense]):\n",
    "                self.layers.append(\n",
    "                    DenseTranspose(\n",
    "                        dense = layer,\n",
    "                        name = kwargs[\"name\"] + \"_{}\".format(layer_index)\n",
    "                        )\n",
    "                    )\n",
    "        else:        \n",
    "            for (layer_index, layer_dim), layer_params_ in zip(\n",
    "                enumerate([x.input_shape[-1] for x in encoder.layers][1:][::-1]), \n",
    "                layers_params):\n",
    "                \n",
    "                if layer_params_[\"dropout\"] > 0.0:\n",
    "                    self.layers.append(Dropout(\n",
    "                        layer_params_[\"dropout\"],\n",
    "                        name = kwargs[\"name\"] + \"_{}_dropout\".format(layer_index)\n",
    "                        ))\n",
    "\n",
    "                self.layers.append(Dense(\n",
    "                    units = layer_dim,\n",
    "                    kernel_initializer= layer_params_[\"kernel_initializer\"],\n",
    "                    bias_initializer= layer_params_[\"bias_initializer\"],\n",
    "                    kernel_regularizer = tf.keras.regularizers.L1L2(\n",
    "                        l1=layer_params_[\"l1\"], \n",
    "                        l2=layer_params_[\"l2\"]),\n",
    "                    name = kwargs[\"name\"] + \"_{}\".format(layer_index)))\n",
    "    \n",
    "    \"\"\"\n",
    "    Add privileged training: training = None \n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = Input(shape=(self.output_dim, ), name = 'decoder_input')\n",
    "        return tf.keras.Model(inputs = [x], outputs = self.call(x))\n",
    "\n",
    "    \"\"\"\n",
    "    Overtide this method to enable serialization \n",
    "    \"\"\"\n",
    "    def get_config(self):\n",
    "        pass\n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Autoencoder\n",
    "    Stack both encoder and decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "    widths : List[int] = [32,28,25], \n",
    "    name : Optional[str] = \"autoencoder\", \n",
    "    ekwargs : Optional[dict] = {}, \n",
    "    dkwargs : Optional[dict] = {}, \n",
    "    **kwargs):\n",
    "        if not (\"name\" in kwargs.keys()):\n",
    "            kwargs[\"name\"] = \"autoencoder\"\n",
    "\n",
    "        super(Autoencoder, self).__init__(**kwargs)\n",
    "        self.input_dim = widths[0]\n",
    "        self.latent_dim = widths[-1]\n",
    "        self.encoder = Encoder(widths, **ekwargs).build_graph()\n",
    "        self.decoder = Decoder(self.encoder, **dkwargs).build_graph()\n",
    "    \n",
    "    def call(self, input):        \n",
    "        x = self.encoder.layers[1](input)\n",
    "        for layer in self.encoder.layers[2:] + self.decoder.layers[1:]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def compile(self, **kwargs):\n",
    "        super(Autoencoder, self).compile(**kwargs)\n",
    "\n",
    "    def build_graph(self,):\n",
    "        x = Input(shape=(self.input_dim, ), name = 'autoencoder_input')\n",
    "        return tf.keras.Model(inputs = [x], outputs = self.call(x))\n",
    "\n",
    "    def encode(self, input):\n",
    "        return self.encoder(input)\n",
    "    \n",
    "    def decode(self, input):\n",
    "        return self.decoder(input)\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.input_dim\n",
    "    \n",
    "    def get_latent_dim(self) -> int:\n",
    "        return self.latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "autoencoder_input (InputLaye [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "encoder_0_dropout (Dropout)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 30)                990       \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 28)                868       \n",
      "_________________________________________________________________\n",
      "encoder_2_dropout (Dropout)  (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 26)                754       \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 20)                540       \n",
      "_________________________________________________________________\n",
      "decoder_0 (DenseTranspose)   (None, 26)                566       \n",
      "_________________________________________________________________\n",
      "decoder_1 (DenseTranspose)   (None, 28)                782       \n",
      "_________________________________________________________________\n",
      "decoder_2 (DenseTranspose)   (None, 30)                898       \n",
      "_________________________________________________________________\n",
      "decoder_3 (DenseTranspose)   (None, 32)                1022      \n",
      "=================================================================\n",
      "Total params: 3,268\n",
      "Trainable params: 3,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder([32, 30, 28, 26, 20])\n",
    "model.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "decoder_0 (DenseTranspose)   (None, 26)                566       \n",
      "_________________________________________________________________\n",
      "decoder_1 (DenseTranspose)   (None, 28)                782       \n",
      "_________________________________________________________________\n",
      "decoder_2 (DenseTranspose)   (None, 30)                898       \n",
      "_________________________________________________________________\n",
      "decoder_3 (DenseTranspose)   (None, 32)                1022      \n",
      "=================================================================\n",
      "Total params: 3,268\n",
      "Trainable params: 3,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Encoder([32, 30, 28, 26, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoder.encoder_layers[1]) == Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
