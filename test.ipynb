{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import Autoencoder\n",
    "from SINDY import sindy_library_tf\n",
    "from HIFF import generate_training_sat\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] generating trainnig dataset...\n",
      "[INFO] generating testing dataset...\n"
     ]
    }
   ],
   "source": [
    "# generate the trainig set \n",
    "print(\"[INFO] generating trainnig dataset...\")\n",
    "(trainX, trainY) = generate_training_sat(32, 100)\n",
    "\n",
    "# generate the test set \n",
    "print(\"[INFO] generating testing dataset...\")\n",
    "(testX, testY) = generate_training_sat(32,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "autoencoder_input (InputLaye [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "encoder_0_dropout (Dropout)  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 25)                825       \n",
      "_________________________________________________________________\n",
      "decoder_0 (DenseTranspose)   (None, 32)                857       \n",
      "=================================================================\n",
      "Total params: 857\n",
      "Trainable params: 857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "20/20 [==============================] - 1s 4ms/step - loss: 0.3159\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2637\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.2382\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.2026\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1734\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1539\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1388\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1279\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1173\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1137\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1058\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1045\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0933\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0951\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0800\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0742\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0722\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0702\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0749\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0770\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0676\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0761\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.0654\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0641\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0647\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0598\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0722\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0668\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0641\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.0670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f89f8495278>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 5\n",
    "model = Autoencoder([32, 25], ekwargs = {\n",
    "        \"layers_params\" : [{\"l1\": 0.001, \"l2\" : 0.0, \"dropout\" : 0.2}],\n",
    "        'layers_default_params' : {\n",
    "        'l1' : 0.001, \n",
    "        'l2' : 0.0, \n",
    "        'dropout' : 0.2, \n",
    "        'activation' : \"tanh\",\n",
    "        'kernel_initializer' : tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "        'bias_initializer' : tf.keras.initializers.Zeros()\n",
    "        },\n",
    "})\n",
    "model.build_graph().summary()\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), \n",
    "        loss=\"mse\")\n",
    "\n",
    "model.fit(trainY, trainY, \n",
    "epochs = epochs, \n",
    "batch_size = batch_size, \n",
    "verbose = 1)\n",
    "\n",
    "plt.imshow(model.encoder.weights[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f89f67cd198>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(model.decoder.weights[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from typing import List, Optional\n",
    "\n",
    "def suplement_layers_params(architecture : List[int], layers_params : dict, layers_default_params : dict) -> dict:\n",
    "    # add empty dicionaries to layers_params to corecponds to the encoding_layers_size size\n",
    "    for _ in range(len(architecture) - len(layers_params)):\n",
    "        layers_params.append({})\n",
    "    return [{**layers_default_params, **x} for x in layers_params]\n",
    "\n",
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Dense transpose layer from dense layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dense, activation = None, **kwargs):\n",
    "        self.dense = dense \n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        super(DenseTranspose, self).__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.b = self.add_weight(name= \"bias\", shape = [ self.dense.input_shape[-1]], initializer = \"zeros\")\n",
    "        self.w = self.dense.weights[0]\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z = tf.linalg.matmul(inputs, self.w, transpose_b = True)\n",
    "        return self.activation(z + self.b)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {\"w\": np.shape(tf.transpose(self.w))}    \n",
    "    @property \n",
    "    def weights_transpose(self):\n",
    "        return tf.transpose(self.dense.weights[0])\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "\n",
    "    isFirstInputLayer - use input dimension only in Input Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "    widths : List[int] = [32,24], \n",
    "    isFirstInputLayer : Optional[bool] = True,\n",
    "    layers_params : Optional[List[dict]] = [\n",
    "        {'l1' : 0.01, 'l2' : 0.0, 'dropout' : 0.2}, \n",
    "        {}\n",
    "        ],\n",
    "    layers_default_params : dict = {\n",
    "        'l1' : 0.01, \n",
    "        'l2' : 0.01, \n",
    "        'dropout' : 0.2, \n",
    "        'activation' : \"tanh\",\n",
    "        'kernel_initializer' : tf.keras.initializers.GlorotUniform(),\n",
    "        'bias_initializer' : tf.keras.initializers.Zeros()\n",
    "        },\n",
    "    **kwargs):\n",
    "        print(\"layers: \" + str(layers_params))\n",
    "        print(\"default: \" + str(layers_default_params))\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.latent_dim = widths[-1]\n",
    "        self.input_dim = widths[0]\n",
    "        self.layers = []\n",
    "        \n",
    "        if not (\"name\" in kwargs.keys()):\n",
    "            kwargs[\"name\"] = \"encoder\"\n",
    "\n",
    "        layers_params = suplement_layers_params(widths, layers_params, layers_default_params)\n",
    "        \n",
    "        for (layer_index, layer_dim), layer_params_ in zip(\n",
    "            enumerate(widths[1:] if isFirstInputLayer else widths), \n",
    "            layers_params):\n",
    "\n",
    "            if layer_params_[\"dropout\"] > 0.0:\n",
    "                self.layers.append(Dropout(\n",
    "                    layer_params_[\"dropout\"],\n",
    "                    name = kwargs[\"name\"] + \"_{}_dropout\".format(layer_index)\n",
    "                    ))\n",
    "            # construct encoder layer \n",
    "            self.layers.append(Dense(\n",
    "                    units = layer_dim,\n",
    "                    kernel_initializer= layer_params_[\"kernel_initializer\"],\n",
    "                    bias_initializer= layer_params_[\"bias_initializer\"],\n",
    "                    kernel_regularizer = tf.keras.regularizers.L1L2(\n",
    "                        l1=layer_params_[\"l1\"], \n",
    "                        l2=layer_params_[\"l2\"]),\n",
    "                    name = kwargs[\"name\"] + \"_{}\".format(layer_index)))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = Input(shape=(self.input_dim, ), name = 'encoder_input')\n",
    "        return tf.keras.Model(inputs = [x], outputs = self.call(x))\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Decoder\n",
    "\n",
    "    isTranspose - use transposed layers from encoder. If False create fresh layers of the same size of encoder\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Add decoder documentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "    encoder : Encoder, \n",
    "    isTranspose : Optional[bool] = True,  \n",
    "    layers_params : Optional[List[dict]] = [],\n",
    "    layers_default_params : dict = {\n",
    "        'l1' : 0.0, \n",
    "        'l2' : 0.0,\n",
    "        'dropout' : 0.0, \n",
    "        'activation' : \"tanh\",\n",
    "        'kernel_initializer' : tf.keras.initializers.GlorotUniform(),\n",
    "        'bias_initializer' : tf.keras.initializers.Zeros()\n",
    "        }, \n",
    "    **kwargs):\n",
    "\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.input_dim = encoder.weights[0].shape[-1]\n",
    "        self.output_dim = encoder.weights[-1].shape[-1]\n",
    "        self.layers = []\n",
    "\n",
    "        if not (\"name\" in kwargs.keys()):\n",
    "            kwargs[\"name\"] = \"decoder\"\n",
    "\n",
    "        layers_params = suplement_layers_params([x for x in encoder.layers[1:][::-1] if type(x) == Dense], layers_params, layers_default_params)\n",
    "\n",
    "        if isTranspose:\n",
    "            for layer_index, layer in enumerate([x for x in encoder.layers[1:][::-1] if type(x) == Dense]):\n",
    "                self.layers.append(\n",
    "                    DenseTranspose(\n",
    "                        dense = layer,\n",
    "                        name = kwargs[\"name\"] + \"_{}\".format(layer_index)\n",
    "                        )\n",
    "                    )\n",
    "        else:        \n",
    "            for (layer_index, layer_dim), layer_params_ in zip(\n",
    "                enumerate([x.input_shape[-1] for x in encoder.layers][1:][::-1]), \n",
    "                layers_params):\n",
    "                \n",
    "                if layer_params_[\"dropout\"] > 0.0:\n",
    "                    self.layers.append(Dropout(\n",
    "                        layer_params_[\"dropout\"],\n",
    "                        name = kwargs[\"name\"] + \"_{}_dropout\".format(layer_index)\n",
    "                        ))\n",
    "\n",
    "                self.layers.append(Dense(\n",
    "                    units = layer_dim,\n",
    "                    kernel_initializer= layer_params_[\"kernel_initializer\"],\n",
    "                    bias_initializer= layer_params_[\"bias_initializer\"],\n",
    "                    kernel_regularizer = tf.keras.regularizers.L1L2(\n",
    "                        l1=layer_params_[\"l1\"], \n",
    "                        l2=layer_params_[\"l2\"]),\n",
    "                    name = kwargs[\"name\"] + \"_{}\".format(layer_index)))\n",
    "    \n",
    "    \"\"\"\n",
    "    Add privileged training: training = None \n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = Input(shape=(self.output_dim, ), name = 'decoder_input')\n",
    "        return tf.keras.Model(inputs = [x], outputs = self.call(x))\n",
    "\n",
    "    \"\"\"\n",
    "    Overtide this method to enable serialization \n",
    "    \"\"\"\n",
    "    def get_config(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Autoencoder\n",
    "    Stack both encoder and decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "    widths : List[int] = [32,28,25], \n",
    "    name : Optional[str] = \"autoencoder\", \n",
    "    ekwargs : Optional[dict] = {}, \n",
    "    dkwargs : Optional[dict] = {}, \n",
    "    **kwargs):\n",
    "        if not (\"name\" in kwargs.keys()):\n",
    "            kwargs[\"name\"] = \"autoencoder\"\n",
    "\n",
    "        super(Autoencoder, self).__init__(**kwargs)\n",
    "        self.input_dim = widths[0]\n",
    "        self.latent_dim = widths[-1]\n",
    "        self.encoder = Encoder(widths, **ekwargs).build_graph()\n",
    "        self.decoder = Decoder(self.encoder, **dkwargs).build_graph()\n",
    "    \n",
    "    def call(self, input):        \n",
    "        x = self.encoder.layers[1](input)\n",
    "        for layer in self.encoder.layers[2:] + self.decoder.layers[1:]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def compile(self, **kwargs):\n",
    "        super(Autoencoder, self).compile(**kwargs)\n",
    "\n",
    "    def build_graph(self,):\n",
    "        x = Input(shape=(self.input_dim, ), name = 'autoencoder_input')\n",
    "        return tf.keras.Model(inputs = [x], outputs = self.call(x))\n",
    "\n",
    "    def encode(self, input):\n",
    "        return self.encoder(input)\n",
    "    \n",
    "    def decode(self, input):\n",
    "        return self.decoder(input)\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.input_dim\n",
    "    \n",
    "    def get_latent_dim(self) -> int:\n",
    "        return self.latent_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
