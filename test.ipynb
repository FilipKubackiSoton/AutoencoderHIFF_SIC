{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "from autoencoder import Autoencoder\n",
    "from SINDY import sindy_library_tf\n",
    "from HIFF import generate_training_sat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] generating trainnig dataset...\n",
      "[INFO] generating testing dataset...\n"
     ]
    }
   ],
   "source": [
    "# generate the trainig set \n",
    "print(\"[INFO] generating trainnig dataset...\")\n",
    "(trainX, trainY) = generate_training_sat(32, 300)\n",
    "\n",
    "# generate the test set \n",
    "print(\"[INFO] generating testing dataset...\")\n",
    "(testX, testY) = generate_training_sat(32,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "autoencoder_input (InputLaye [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "fucku_0_dropout (Dropout)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "fucku_0 (Dense)              (None, 25)                825       \n",
      "_________________________________________________________________\n",
      "decoder_0 (DenseTranspose)   (None, 32)                857       \n",
      "=================================================================\n",
      "Total params: 857\n",
      "Trainable params: 857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "Train step\n",
      "Train step\n",
      "60/60 [==============================] - 1s 3ms/step - loss: 0.2792\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.1974\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1442\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.1127\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0903\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0814\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0724\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0714\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0690\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0661\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0617\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0619\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0649\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0637\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0654\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0654\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0617\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0618\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0620\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 0s 3ms/step - loss: 0.0625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f84b05d25f8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 5\n",
    "# \"layers_params\" : [{\"l1\": 0.001, \"l2\" : 0.0, \"dropout\" : 0.2}, {\"l1\" : 0.001}],\n",
    "\n",
    "model = Autoencoder([32, 25], ekwargs = {\n",
    "        \"name\" : \"fucku\",\n",
    "        \"layers_params\" : [{\"l1\": 0.001, \"l2\" : 0.0, \"dropout\" : 0.2}, {\"l1\" : 0.001}],\n",
    "        'layers_default_params' : {\n",
    "        'l1' : 0.001, \n",
    "        'l2' : 0.0, \n",
    "        'dropout' : 0.2, \n",
    "        'activation' : \"tanh\",\n",
    "        'kernel_initializer' : tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "        'bias_initializer' : tf.keras.initializers.Zeros()\n",
    "        },\n",
    "})\n",
    "model.build_graph().summary()\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), \n",
    "        loss=\"mse\")\n",
    "\n",
    "model.fit(trainY, trainY, \n",
    "epochs = epochs, \n",
    "batch_size = batch_size, \n",
    "verbose = 1)\n",
    "\n",
    "plt.imshow(model.encoder.weights[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "autoencoder_input (InputLaye [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "fucku_0_dropout (Dropout)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "fucku_0 (Dense)              (None, 25)                825       \n",
      "_________________________________________________________________\n",
      "decoder_0 (DenseTranspose)   (None, 32)                857       \n",
      "=================================================================\n",
      "Total params: 857\n",
      "Trainable params: 857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder([32, 25], ekwargs = {\n",
    "        \"name\" : \"fucku\",\n",
    "        \"layers_params\" : [{\"l1\": 0.001, \"l2\" : 0.0, \"dropout\" : 0.2}, {\"l1\" : 0.001}],\n",
    "        'layers_default_params' : {\n",
    "        'l1' : 0.001, \n",
    "        'l2' : 0.0, \n",
    "        'dropout' : 0.2, \n",
    "        'activation' : \"tanh\",\n",
    "        'kernel_initializer' : tf.keras.initializers.RandomNormal(stddev=0.01),\n",
    "        'bias_initializer' : tf.keras.initializers.Zeros()\n",
    "        },\n",
    "})\n",
    "model.build_graph().summary()\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), \n",
    "        loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((trainY, trainY))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((testY, testY))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-1.0958582 ,  0.06148767],\n",
       "       [-0.80121917, -0.8994139 ]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {}\n",
    "params['coefficient_initialization'] = tf.keras.initializers.GlorotUniform()(shape = (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- network - finished\n",
    "- loss - still to code up\n",
    "- optimizer - set by default \n",
    "- refinement optimizer - not implemented yet\n",
    "- saver - to add in the future with custom callback (see custom models + layers - overide seralization in thesea autoencoder + encoder + decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.5472\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.7507\n",
      "Validation acc: 0.8672\n",
      "Time taken: 1.32s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.1137\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.8635\n",
      "Validation acc: 0.9153\n",
      "Time taken: 1.31s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.0750\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.8707\n",
      "Validation acc: 0.8861\n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 0.0759\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.8774\n",
      "Validation acc: 0.8932\n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 0.0640\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.8823\n",
      "Validation acc: 0.8798\n",
      "Time taken: 1.35s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 0.0758\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.8976\n",
      "Validation acc: 0.9183\n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 0.0511\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.8936\n",
      "Validation acc: 0.9067\n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 0.0508\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9002\n",
      "Validation acc: 0.9079\n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 0.0498\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9024\n",
      "Validation acc: 0.8990\n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.0785\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9066\n",
      "Validation acc: 0.9526\n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 0.0288\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9218\n",
      "Validation acc: 0.9391\n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 0.0624\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9260\n",
      "Validation acc: 0.9019\n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 0.0628\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9334\n",
      "Validation acc: 0.9563\n",
      "Time taken: 0.62s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 0.0291\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9387\n",
      "Validation acc: 0.9629\n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 0.0393\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9463\n",
      "Validation acc: 0.9712\n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 0.0214\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9487\n",
      "Validation acc: 0.9774\n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 0.0234\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9419\n",
      "Validation acc: 0.9576\n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 0.0294\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9502\n",
      "Validation acc: 0.9420\n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 0.0292\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9477\n",
      "Validation acc: 0.9748\n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 0.0205\n",
      "Seen so far: 5 samples\n",
      "Training acc over epoch: 0.9496\n",
      "Validation acc: 0.9810\n",
      "Time taken: 0.59s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "train_acc_metric = tf.keras.metrics.Mean()\n",
    "val_acc_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f84b0c0e1d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(model.encoder.weights[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Autoencoder\n",
    "    Stack both encoder and decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "    widths : List[int] = [32,28,25], \n",
    "    ekwargs : Optional[dict] = {}, \n",
    "    dkwargs : Optional[dict] = {}, \n",
    "    **kwargs):\n",
    "        if not (\"name\" in kwargs.keys()):\n",
    "            kwargs[\"name\"] = \"autoencoder\"\n",
    "\n",
    "        super(Autoencoder, self).__init__(**kwargs)\n",
    "        self.input_dim = widths[0]\n",
    "        self.latent_dim = widths[-1]\n",
    "        self.encoder = Encoder(widths, **ekwargs).build_graph()\n",
    "        self.decoder = Decoder(self.encoder, **dkwargs).build_graph()\n",
    "    \n",
    "    def call(self, input):        \n",
    "        x = self.encoder.layers[1](input)\n",
    "        for layer in self.encoder.layers[2:] + self.decoder.layers[1:]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        \n",
    "    \"\"\" def train_step(self, data):\n",
    "\n",
    "        print(\"Train step\")\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \"\"\"\n",
    "    def compile(self, **kwargs):\n",
    "        super(Autoencoder, self).compile(**kwargs)\n",
    "\n",
    "    def build_graph(self,):\n",
    "        x = Input(shape=(self.input_dim, ), name = 'autoencoder_input')\n",
    "        return tf.keras.Model(inputs = [x], outputs = self.call(x))\n",
    "\n",
    "    def encode(self, input):\n",
    "        return self.encoder(input)\n",
    "    \n",
    "    def decode(self, input):\n",
    "        return self.decoder(input)\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.input_dim\n",
    "    \n",
    "    def get_latent_dim(self) -> int:\n",
    "        return self.latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from typing import List, Optional\n",
    "\n",
    "def suplement_layers_params(architecture : List[int], layers_params : List[dict], layers_default_params : dict) -> dict:\n",
    "    # add empty dicionaries to layers_params to corecponds to the encoding_layers_size size\n",
    "    for _ in range(len(architecture) - len(layers_params)):\n",
    "        layers_params.append({})\n",
    "    return [{**layers_default_params, **x} for x in layers_params]\n",
    "\n",
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Dense transpose layer from dense layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dense, activation = None, **kwargs):\n",
    "        self.dense = dense \n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        super(DenseTranspose, self).__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.b = self.add_weight(name= \"bias\", shape = [ self.dense.input_shape[-1]], initializer = \"zeros\")\n",
    "        self.w = self.dense.weights[0]\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z = tf.linalg.matmul(inputs, self.w, transpose_b = True)\n",
    "        return self.activation(z + self.b)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {\"w\": np.shape(tf.transpose(self.w))}    \n",
    "    @property \n",
    "    def weights_transpose(self):\n",
    "        return tf.transpose(self.dense.weights[0])\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "\n",
    "    isFirstInputLayer - use input dimension only in Input Layer\n",
    "    Kernel and bias are initialize by fresh instances of objects\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "    widths : List[int] = [32,24], \n",
    "    isFirstInputLayer : Optional[bool] = True,\n",
    "    layers_params : Optional[List[dict]] = [{}],\n",
    "    layers_default_params : dict = {\n",
    "        'l1' : 0.001, \n",
    "        'l2' : 0.0, \n",
    "        'dropout' : 0.2, \n",
    "        'activation' : \"tanh\",\n",
    "        'kernel_initializer' : tf.keras.initializers.GlorotUniform(),\n",
    "        'bias_initializer' : tf.keras.initializers.Zeros()\n",
    "        },\n",
    "    **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.latent_dim = widths[-1]\n",
    "        self.input_dim = widths[0]\n",
    "        self.layers = []\n",
    "        \n",
    "        if not (\"name\" in kwargs.keys()):\n",
    "            kwargs[\"name\"] = \"encoder\"\n",
    "\n",
    "        layers_params = suplement_layers_params(widths, layers_params, layers_default_params)\n",
    "        for (layer_index, layer_dim), layer_params_ in zip(\n",
    "            enumerate(widths[1:] if isFirstInputLayer else widths), \n",
    "            layers_params):\n",
    "\n",
    "            if layer_params_[\"dropout\"] > 0.0:\n",
    "                self.layers.append(Dropout(\n",
    "                    layer_params_[\"dropout\"],\n",
    "                    name = kwargs[\"name\"] + \"_{}_dropout\".format(layer_index)\n",
    "                    ))\n",
    "            # construct encoder layer \n",
    "            self.layers.append(Dense(\n",
    "                    units = layer_dim,\n",
    "                    activation= layer_params_[\"activation\"],\n",
    "                    kernel_initializer= layer_params_[\"kernel_initializer\"].__class__(**layer_params_[\"kernel_initializer\"].get_config()),\n",
    "                    bias_initializer= layer_params_[\"bias_initializer\"].__class__(**layer_params_[\"bias_initializer\"].get_config()),\n",
    "                    kernel_regularizer = tf.keras.regularizers.L1L2(\n",
    "                        l1=layer_params_[\"l1\"], \n",
    "                        l2=layer_params_[\"l2\"]),\n",
    "                    name = kwargs[\"name\"] + \"_{}\".format(layer_index)))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = Input(shape=(self.input_dim, ), name = 'encoder_input')\n",
    "        return tf.keras.Model(inputs = [x], outputs = self.call(x))\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Decoder\n",
    "\n",
    "    isTranspose - use transposed layers from encoder. If False create fresh layers of the same size of encoder\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Add decoder documentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "    encoder : Encoder, \n",
    "    isTranspose : Optional[bool] = True,  \n",
    "    layers_params : Optional[List[dict]] = [],\n",
    "    layers_default_params : dict = {\n",
    "        'l1' : 0.0, \n",
    "        'l2' : 0.0,\n",
    "        'dropout' : 0.0, \n",
    "        'activation' : \"tanh\",\n",
    "        'kernel_initializer' : tf.keras.initializers.GlorotUniform(),\n",
    "        'bias_initializer' : tf.keras.initializers.Zeros()\n",
    "        }, \n",
    "    **kwargs):\n",
    "\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.input_dim = encoder.weights[0].shape[-1]\n",
    "        self.output_dim = encoder.weights[-1].shape[-1]\n",
    "        self.layers = []\n",
    "\n",
    "        if not (\"name\" in kwargs.keys()):\n",
    "            kwargs[\"name\"] = \"decoder\"\n",
    "\n",
    "        layers_params = suplement_layers_params([x for x in encoder.layers[1:][::-1] if type(x) == Dense], layers_params, layers_default_params)\n",
    "\n",
    "        if isTranspose:\n",
    "            for layer_index, layer in enumerate([x for x in encoder.layers[1:][::-1] if type(x) == Dense]):\n",
    "                self.layers.append(\n",
    "                    DenseTranspose(\n",
    "                        dense = layer,\n",
    "                        name = kwargs[\"name\"] + \"_{}\".format(layer_index)\n",
    "                        )\n",
    "                    )\n",
    "        else:        \n",
    "            for (layer_index, layer_dim), layer_params_ in zip(\n",
    "                enumerate([x.input_shape[-1] for x in encoder.layers][1:][::-1]), \n",
    "                layers_params):\n",
    "                \n",
    "                if layer_params_[\"dropout\"] > 0.0:\n",
    "                    self.layers.append(Dropout(\n",
    "                        layer_params_[\"dropout\"],\n",
    "                        name = kwargs[\"name\"] + \"_{}_dropout\".format(layer_index)\n",
    "                        ))\n",
    "\n",
    "                self.layers.append(Dense(\n",
    "                    units = layer_dim,\n",
    "                    activation= layer_params_[\"activation\"],\n",
    "                    kernel_initializer= layer_params_[\"kernel_initializer\"].__class__(**layer_params_[\"kernel_initializer\"].get_config()),\n",
    "                    bias_initializer= layer_params_[\"bias_initializer\"].__class__(**layer_params_[\"bias_initializer\"].get_config()),\n",
    "                    kernel_regularizer = tf.keras.regularizers.L1L2(\n",
    "                        l1=layer_params_[\"l1\"], \n",
    "                        l2=layer_params_[\"l2\"]),\n",
    "                    name = kwargs[\"name\"] + \"_{}\".format(layer_index)))\n",
    "    \n",
    "    \"\"\"\n",
    "    Add privileged training: training = None \n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = Input(shape=(self.output_dim, ), name = 'decoder_input')\n",
    "        return tf.keras.Model(inputs = [x], outputs = self.call(x))\n",
    "\n",
    "    \"\"\"\n",
    "    Overtide this method to enable serialization \n",
    "    \"\"\"\n",
    "    def get_config(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
